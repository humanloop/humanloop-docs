## Groq support (Beta)

We have introduced support for models available on Groq to Humanloop. You can now try out the blazingly fast generations made with the open-source models (such as Llama 3 and Mixtral 8x7B) hosted on Groq within our Prompt Editor.

<img src="../../../assets/images/43bc0ec-Isolated_image_2.png" />


Groq achieves [faster throughput](https://artificialanalysis.ai/models/llama-3-instruct-70b/providers)  using specialized hardware, their LPU Inference Engine. More information is available in their [FAQ](https://wow.groq.com/why-groq/) and on their website.

<br />

Note that their API service, GroqCloud, is still in beta and low rate limits are enforced.