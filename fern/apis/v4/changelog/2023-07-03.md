## Introducing Tools

Today weâ€™re announcing Tools as a part of Humanloop. 

Tools allow you to connect an LLM to any API and to an array of data sources to give it extra capabilities and access to private data. Under your organization settings on Humanloop you can now configure and manage tools in a central place.

Read more on [our blog](https://humanloop.com/blog/announcing-tools) and see an example of setting up a [tool for semantic search](https://docs.humanloop.com/docs/setup-semantic-search).

## OpenAI functions API

We've updated our APIs to support [OpenAI function calling](https://platform.openai.com/docs/guides/gpt/function-calling). 

OpenAI functions are now supported as tools on Humanloop. This allows you to pass tool definitions as part of the model configuration when calling our `chat` and `log` endpoints. For the latest OpenAI models `gpt-3.5-turbo-0613` and `gpt-4-0613` the model can then choose to output a JSON object containing arguments to call these tools. 

This unlocks getting more reliable structured data back from the model and makes it easier to create useful agents.

### Recap on OpenAI functions

As described in the [OpenAI documentation](https://platform.openai.com/docs/guides/gpt/function-calling), the basic steps for using functions are:

1. Call one of the models `gpt-3.5-turbo-0613` and `gpt-4-0613` with a user query and a set of function definitions described using the universal [json-schema](https://json-schema.org/) syntax.
2. The model can then choose to call one of the functions provided. If it does, a stringified JSON object adhering to your json schema definition will be returned. 
3. You can then parse the string into JSON in your code and call the chosen function with the provided arguments (**NB:** the model may hallucinate or return invalid json, be sure to consider these scenarios in your code).
4. Finally call the model again by appending the function response as a new message. The model can then use this information to respond to the original use query.

OpenAI have provided a simple example in their docs for a `get_current_weather` function that we will show how to adapt to use with Humanloop:

```python
import openai
import json


# Example dummy function hard coded to return the same weather
# In production, this could be your backend API or an external API
def get_current_weather(location, unit="fahrenheit"):
    """Get the current weather in a given location"""
    weather_info = {
        "location": location,
        "temperature": "72",
        "unit": unit,
        "forecast": ["sunny", "windy"],
    }
    return json.dumps(weather_info)


def run_conversation():
    # Step 1: send the conversation and available functions to GPT
    messages = [{"role": "user", "content": "What's the weather like in Boston?"}]
    functions = [
        {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        }
    ]
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-0613",
        messages=messages,
        functions=functions,
        function_call="auto",  # auto is default, but we'll be explicit
    )
    response_message = response["choices"][0]["message"]

    # Step 2: check if GPT wanted to call a function
    if response_message.get("function_call"):
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            "get_current_weather": get_current_weather,
        }  # only one function in this example, but you can have multiple
        function_name = response_message["function_call"]["name"]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message["function_call"]["arguments"])
        function_response = fuction_to_call(
            location=function_args.get("location"),
            unit=function_args.get("unit"),
        )

        # Step 4: send the info on the function call and function response to GPT
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                "role": "function",
                "name": function_name,
                "content": function_response,
            }
        )  # extend conversation with function response
        second_response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-0613",
            messages=messages,
        )  # get a new response from GPT where it can see the function response
        return second_response


print(run_conversation())
```

### Using with Humanloop tools

OpenAI functions are treated as tools on Humanloop. Tools conveniently follow the same universal json-schema definition as OpenAI functions. 

We've expanded the definition of our model configuration to also include tool definitions. Historically the model config is made up of the chat template, choice of base model and any hyper-parameters that change the behaviour of the model.

In the cases of OpenAIs `gpt-3.5-turbo-0613` and `gpt-4-0613` models, any tools defined as part of the model config are passed through as functions for the model to use. 

You can now specify these tools when using the Humanloop chat endpoint (as a replacement for OpenAI's ChatCompletion), or when using the Humanloop log endpoint in addition to the OpenAI calls:

#### Chat endpoint

We show here how to update the `run_conversation()` method from the OpenAI example to instead use the Humanloop chat endpoint with tools:

```python
from humanloop import Humanloop

hl = Humanloop(
  	# get your API key here: https://app.humanloop.com/account/api-keys
    api_key="YOUR_API_KEY",
)

def run_conversation():
    # Step 1: send the conversation and available functions to GPT
    messages = [{"role": "user", "content": "What's the weather like in Boston?"}]
    # functions are referred to as tools on Humanloop, but follows the same schema
		tools = [
        {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        }
    ]
    response = hl.chat(
      project="Assistant",
      model_config={
        "model": "gpt-3.5-turbo-0613",
      	"tools": tools
      },
      messages=messages
    )
    response = response.body["data"][0]

    # Step 2: check if GPT wanted to call a tool
    if response.get("tool_call"):
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            "get_current_weather": get_current_weather,
        }  # only one function in this example, but you can have multiple
        function_name = response_message["function_call"]["name"]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response["tool_call"]["arguments"])
        function_response = fuction_to_call(
            location=function_args.get("location"),
            unit=function_args.get("unit"),
        )

        # Step 4: send the response back to the model
        messages.append(response_message)  
        messages.append(
            {
                "role": "tool",
                "name": function_name,
                "content": function_response,
            }
        ) 
        second_response = hl.chat(
          project="Assistant",
          model_config={
            "model": "gpt-3.5-turbo-0613",
            "tools": tools
          },
          messages=messages
        )
        return second_response
```

After running this snippet, the model configuration recorded on your project in Humanloop will now track what tools were provided to the model and the logged datapoints will provide details of the tool called to inspect:

![](https://files.readme.io/1b48140-Screenshot_2023-07-03_at_06.42.56.png)

#### Log endpoint

Alternatively, you can also use the explicit Humanloop log alongside your existing OpenAI calls to achieve the same result: 

```python
from humanloop import Humanloop

hl = Humanloop(
  	# get your API key here: https://app.humanloop.com/account/api-keys
    api_key="YOUR_API_KEY",
)

def run_conversation():
    # Step 1: send the conversation and available functions to GPT
    messages = [{"role": "user", "content": "What's the weather like in Boston?"}]
    functions = [
        {
            "name": "get_current_weather",
            "description": "Get the current weather in a given location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "The city and state, e.g. San Francisco, CA",
                    },
                    "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]},
                },
                "required": ["location"],
            },
        }
    ]
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo-0613",
        messages=messages,
        functions=functions,
        function_call="auto",  # auto is default, but we'll be explicit
    )
    response_message = response["choices"][0]["message"]
		
		# log the result to humanloop
    log_response = hl.log(
       project="Assistant",
          model_config={
            "model": "gpt-3.5-turbo-0613",
            "tools": tools,
          },
          messages=messages,
      		tool_call=response_message.get("function_call")
    )
		
    # Step 2: check if GPT wanted to call a function
    if response_message.get("function_call"):
        # Step 3: call the function
        # Note: the JSON response may not always be valid; be sure to handle errors
        available_functions = {
            "get_current_weather": get_current_weather,
        }  # only one function in this example, but you can have multiple
        function_name = response_message["function_call"]["name"]
        fuction_to_call = available_functions[function_name]
        function_args = json.loads(response_message["function_call"]["arguments"])
        function_response = fuction_to_call(
            location=function_args.get("location"),
            unit=function_args.get("unit"),
        )

        # Step 4: send the info on the function call and function response to GPT
        messages.append(response_message)  # extend conversation with assistant's reply
        messages.append(
            {
                "role": "function",
                "name": function_name,
                "content": function_response,
            }
        )  # extend conversation with function response
        second_response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo-0613",
            messages=messages,
        )  # get a new response from GPT where it can see the function response
        
        log_response = hl.log(
          project="Assistant",
          model_config={
                  "model": "gpt-3.5-turbo-0613",
                  "tools": tools,
          },
          messages=messages,
          output=second_response["choices"][0]["message"]["content"],
    )
    return second_response


print(run_conversation())
```

### Coming soon

Support for defining tools in the playground!