## Human Evaluator upgrades

We've made significant upgrades to Human Evaluators and related workflows to improve your ability to gather Human judgements (sometimes referred to as "feedback") in assessing the quality of your AI applications.

Here are some of the key improvements:

- Instead of having to define a limited feedback schema tied to the settings of a specific Prompt, you can now **define your schema with a Human Evaluator file and reuse it across multiple Prompts and Tools** for both monitoring and offline evaluation purposes.
- You are no longer restricted to the default types of `Rating`, `Actions` and `Issues` when defining your feedback schemas from the UI. We've introduced a **more flexible Editor interface supporting different return types** and valence controls.
- We've extended the scope of Human Evaluators so that they can now **also be used with Tools and other Evaluators** (useful for validating AI judgements) in the same way as with Prompts.
- We've **improved the Logs drawer UI for applying feedback** to Logs, in particular making the buttons more responsive. 

![Tone evaluator set up with options and instructions](../assets/images/setup-tone-human-evaluator-no-tip.png)


To set up a Human Evaluator, create a new file. Within the file creation dialog, click on **Evaluator**, then click on **Human**.
This will create a new Human Evaluator file and bring you to its Editor. Here, you can choose a `Return type` for the Evaluator and configure the feedback schema.

You can then reference this Human Evaluator within the `Monitoring` dropdown of Prompts, Tools, and other Evaluators. As well when configuring reports in their `Evaluations` tab.  

We've set up a default `Rating` and `Correction` Evaluators that will be automatically attached to all Prompts new and existing. We've migrated all your existing Prompt specific feedback schemas to Human Evaluator files and these will continue to work as before with no disruption.

Checkout our updated document for further details on how to use Human Evaluators:

- [Create a Human Evaluator](/docs/v5/evaluation/guides/human-evaluator)
- [Capture End User Feedback](/docs/v5/observability/guides/capture-user-feedback)
- [Run a Human Evaluation](/docs/v5/evaluation/guides/run-human-evaluation)
