## Human Evaluators with Judgment Schema

We've introduced the ability to define Human Evaluators with a specific judgment schema. You can now set up a Human Evaluator and use it to capture feedback and reuse across multiple Prompts and Evaluations.

![Tone evaluator set up with options and instructions](../assets/images/setup-tone-human-evaluator-no-tip.png)

Similar to other Evaluators, you can attach a Human Evaluator to a Prompt through the Monitoring dialog.

You can set your Evaluations to use a Human Evaluator. When you do so, you'll be able to provide feedback via the Logs tab of the Evaluation, and view aggregate stats of the related feedback that was provided against the Log under the Stats tab.

When you attach a Human Evaluator to a Prompt, you will be able to apply feedback to those Prompt Logs that abide by the Human Evaluator's judgment schema.

To set up a Human Evaluator, create a new file. Within the file creation dialog, click on **Evaluator**, then click on **Human**.
This will create a new Human Evaluator file and bring you to its Editor. Here, you can choose a "return type" for the Evaluator, and set up the judgment schema.

We've set up a default "rating" and "correction" Evaluators that will be automatically attached to Prompt files.

Any prior Feedback schemas attached to Prompts are now migrated to Human Evaluators, along with any associated feedback data.

More details on how to use Human Evaluators are available in the following guides:

- [Create a Human Evaluator](/docs/v5/evaluation/guides/human-evaluator)
- [Capture User Feedback](/docs/v5/observability/guides/capture-user-feedback)
- [Run a Human Evaluation](/docs/v5/evaluation/guides/run-human-evaluation)

## Clearer Evaluations Tab in Logs

We've given the Log drawer's Evaluation tab a facelift and introduced a new "Run again" feature. You can now clearly see what the results are for each of the connected Evaluators that have been applied to that Log.

This means that it's now easier to view the judgments applied to a Log, and if necessary, re-run code/AI Evaluators
against the Log.

![Log drawer's Evaluation tab with the "Run again" menu open](../assets/images/changelogs/logs-drawer-evaluations.png)
