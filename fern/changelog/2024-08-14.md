## Versioned Human Evaluators with Judgment Schema

We've introduced the ability to define Human Evaluators with a specific judgment schema. You can now set up a Human Evaluator and reuse it across Prompts and Evaluations.

![Tone evaluator set up with options and instructions](../assets/images/setup-tone-human-evaluator-no-tip.png)

To set up a Human Evaluator, create a new file. Within the file creation dialog, click on **Evaluator**, then click on **Human**.
This will create a new Human Evaluator file and bring you to its Editor. Here, you can choose a "return type" for the Evaluator, and set up the judgment schema.

We've set up a default "rating" and "correction" Evaluators that will be automatically attached to Prompt files.
We've also migrated past Feedback Types to Human Evaluators, along with any associated feedback data.

Similar to other Evaluators, you can attach a Human Evaluator to a Prompt through the Monitoring dialog.
When you attach a Human Evaluator to a Prompt, you will be able to apply feedback (related to that Human Evaluator) to those Prompt Logs.

Also, you can attach Human Evaluators to Evaluations. When you do so, you'll be able to provide feedback via the Logs tab of the Evaluation, and view aggregate stats of the related feedback that was provided against the Log under the Stats tab.

More details on how to use Human Evaluators are available in the following guides:

- [Create a Human Evaluator](./human-evaluator)
- [Capture User Feedback](./capture-user-feedback)
- [Run a Human Evaluation](./run-human-evaluation)

## Streamlined Log Drawer's Evaluation tab

We've given the Log drawer's Evaluation tab a facelift and introduced a new "Run again" feature.
This means that it's now easier to view the judgments applied to a Log, and if necessary, re-run code/AI Evaluators
against the Log.

![Log drawer's Evaluation tab with the "Run again" menu open](../assets/images/changelogs/logs-drawer-evaluations.png)
