## Versioned Feedback Schema with Human Evaluators

We've introduced the ability to define Human Evaluator with feedback schema. You can now set up a Human Evaluator and reuse its feedback schema across Prompts and Evaluations.

To set up a Human Evaluator, create a new file. Within the file creation dialog, click on **Evaluator**, then click on **Human**.
This will create a new Human Evaluator file and bring you to its Editor. Here, you can choose a "return type" for the Evaluator, and set up certain areas.

We've set up a default "rating" and "correction" Evaluators that will be automatically attached to Prompt files.

Similar to other Evaluators, you can attach a Human Evaluator to a Prompt through the Monitoring dialog.
When you attach a Human Evaluator to a Prompt, you will be able to apply feedback (related to that Human Evaluator) to those Prompt Logs. (Previously, the feedback you were allowed to give through the Logs drawer was limited to feedback types set up on each individual Prompt.)

Also, you can attach Human Evaluators to Evaluation reports. When you do so, you'll be able to provide feedback via the Logs tab of the Evaluation, and view aggregate stats of the related feedback that was provided against the Log under the Stats tab.

### Creating a Human Evaluator - Guide

### Setting up a Human Evaluator for feedback - Guide

- Set up Human evaluator
- Attach to Prompt
- Give feedback via drawer (and mention ability to give feedback through table cell for categorical feedback)

### Using Feedback in Evaluations

### Migrating from the V4 API to the V5 API

The "Feedback Type" concept in V4 of our API is similar to a Human Evaluator in V5 of our API.

TODO: Example of a V4 vs V5 request.
