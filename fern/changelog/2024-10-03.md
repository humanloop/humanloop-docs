# Evals SDK Improvements

We've added a new `run` method for evals to our SDK. This provides a simpler 
entry point for evaluating your existing pipelines, both in your CICD and experimentation 
workflows. This is currently available in [Beta](https://pypi.org/project/humanloop/0.8.6b2/) on Python and will soon be added to the major versions of both Py and TS SDKs.

In order to run an eval via the SDK, you need to provide:

1. A callable function that takes your inputs/messages and returns a string
2. A [Dataset](https://humanloop.com/docs/evaluation/guides/create-dataset) of inputs/message to evaluate the function against
3. A set of [Evaluators](https://humanloop.com/docs/evaluation/guides/llm-as-a-judge) to use to provide judgements

Here is a toy example using a simple OpenAI call as the function being evaluated.

```python
from humanloop import Humanloop
from openai import OpenAI
from dotenv import load_dotenv
import os

load_dotenv()
hl = Humanloop(api_key=os.getenv("HUMANLOOP_KEY"))
openai = OpenAI(api_key=os.getenv("OPENAI_KEY"))


# First define the app you're evaluating
def call_digital_twin(person: str, messages: list) -> str:
    system_message = {
        "role": "system",
        "content": f"You are {person}"
    }
    chat_completion = openai.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[system_message] + messages,
    )
    answer = chat_completion.choices[0].message.content
    return answer


# Then run an eval specifying the file location on Humanloop
checks = hl.evaluations.run(
    name="Demo eval",
    file={
        "path": "digital-twin",
        "callable": call_digital_twin
    },
    evaluators=[
        {"path": "Latency"},
        {"path": "Cost"},
        {"path": "Correctness-AI"}
    ],
    dataset={
        "path": "questions",
        "datapoints": [
            {
                "inputs": {"person": "Albert Einstein"},
                "messages": [{"role": "user", "content": "What is your most famous theory?"}]
            },
            {
                "inputs": {"person": "Serena Williams"},
                "messages": [{"role": "user", "content": "What trophy did you win most recently?"}]
            },
            {
                "inputs": {"person": "Marie Curie"},
                "messages": [{"role": "user", "content": "What was your greatest scientific achievement?"}]
            },
            {
                "inputs": {"person": "Leonardo da Vinci"},
                "messages": [{"role": "user", "content": "Which of your inventions are you most proud of?"}]
            },
            {
                "inputs": {"person": "Rosa Parks"},
                "messages": [{"role": "user", "content": "What motivated you to refuse giving up your seat?"}]
            }
        ]
    },
)
```

Running this will provide status info and an eval summary in your CLI and a new eval will appear on Humanloop at the 
displayed URL. Running it again under the same `name` will add runs to the existing eval.

```
Navigate to your evals:
https://app.humanloop.com/project/fl_euUV4BHoXqKWqFyZ1YD1o/evaluations/evr_6WhFaHdkbWH8ZaoddzyRD/stats

Running digital-twin flow callable over the Dataset questions using 4 workers
[########################################] 5/5 (100.00%) | DONE

‚è≥ Evaluation Progress
Total Logs: 10/10
Total Judgments: 30/30

üìä Eval results for digital-twin 
+----------------+----------------------------------------+----------------------------------------+
|     Version ID | flv_VIP1eiemqbpWmlsr84BwN (eb37773f39) | flv_VIP1eiemqbpWmlsr84BwN (9de378a165) |
+----------------+----------------------------------------+----------------------------------------+
|          Added |          2024-10-08 03:46:11           |          2024-10-08 03:51:52           |
+----------------+----------------------------------------+----------------------------------------+
|     Evaluators |                                        |                                        |
+----------------+----------------------------------------+----------------------------------------+
|        Latency |                  0.02                  |                  0.015                 |
| Correctness-AI |                  1.0                   |                  1.0                   |
+----------------+----------------------------------------+----------------------------------------+

```

It returns a set of checks you can use to determine whether the eval passed or failed.


### Introduce versioning

The only thing distinguishing different eval runs under the same eval `name` so far is the time stamp they were run. 

It can also be helpful to record what the configuration of your system was when running the eval.

You can include arbitrary config within the `version` field of the `file`. If this 
`version` has been used before, Humanloop will automatically associate it to your run. If the 
config is new, we will automatically create a new version of your file for future reference.

```python
import inspect

checks = hl.evaluations.run(
    name="Demo eval",
    file={
        "path": "digital-twin",
        "callable": call_digital_twin,
        "version":{
            "version":"0.2.4",
            "code": inspect.getsource(call_digital_twin)
        } 
    },
    dataset={...},
    evaluators=[...],
)
```

### Leverage native Prompts
 
Using `hl.evaluations.run(...)` will by default create a Flow on Humanloop. Flows have 
the advantage of being able to represent more complex traces, but can't be run natively 
within the Humanloop Editor. 

It's also possible to adapt the `run` call to instead evaluate [Prompts](https://humanloop.com/docs/concepts/prompts) 
by defining the `type` as prompt and providing valid Prompt params in the `version` field.

```python
checks = hl.evaluations.run(
    file={
        "path": "digital-twin-prompt",
        "type": "prompt",
        "version": {
            "model": "gpt-4o-mini",
            "template": [{"role": "system", "content": f"You are {{person}}"}] 
        }
    },
    dataset={...},
    evaluators=[...],
)
```

### Add Evaluator thresholds

You can also now provide a threshold value for each of your Evaluators. 
If provided, the `checks` return will determine whether the average performance 
of the Evaluator met the threshold or not.

```python
checks = hl.evaluations.run(
    file={...},
    dataset={...},
    evaluators=[
        {"path": "Latency"},
        {"path": "Cost"},
        {"path": "Correctness-AI", "threshold": 0.5}
    ],
)
```
