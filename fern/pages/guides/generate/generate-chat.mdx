---
subtitle: A walkthrough of how to generate chat completions from a large language model with the prompt managed in Humanloop.
description: Learn how to generate chat completions from a large language model and log the results in Humanloop, with managed and versioned prompts.
image: https://humanloop.com/assets/docs/social-image.png
---

The Humanloop Python SDK allows you to easily replace your `openai.ChatCompletions.create()` calls with a `humanloop.chat()` call that, in addition to calling OpenAI to get a response, automatically logs the data to your Humanloop project.

## Prerequisites

- You already have a Prompt â€” if not, please follow our [Prompt creation](/docs/guides/create-prompt) guide first.

<Info>
This guide assumes you're using an OpenAI model. If you want to use other providers or your own model please also look at our [guide to using your own model](./use-your-own-model-provider).
</Info>

<Markdown src="../../../snippets/setup-sdk.mdx" />

## Activate a model

1. Log in to Humanloop and navigate to the **Models** tab of your project.
2. Ensure that the default environment is in green at the top of the dashboard.
   The default environment is mapped to your active deployment.
   If there is no active deployment set, then use the dropdown button for the default environment and select the **Change deployment** option to select one of your existing model configs to use to generate. You also need to confirm the model you config you have deployed is a Chat model. This can be confirmed by clicking on the config in the table and viewing the Endpoint, making sure it says **Chat**.

<img src="../../../assets/images/d7e0083-image.png" />

## Use the SDK to call your model

Now you can use the SDK to generate completions and log the results to your project:

```python
# humanloop.chat_deployed(...) will call the active model config on your project.
# The inputs must match the input of the chat template in your project.
chat_response = humanloop.chat_deployed(
    project_id="YOUR_PROJECT_ID_HERE",
  	 # inputs required by your chat_template - for example your templated system message.
    inputs={"persona": "paul graham from YC"},
  	messages=[
    	  {"role": "user", "content": "How should I think about competition for my startup?"}
    ]
)

# A single call to chat may return multiple outputs.
data_id = chat_response.data[0].id
output = chat_response.data[0].output
print(output)

# You can also access the raw response from OpenAI.
print(chat_response.provider_responses)
```

Navigate to your project's **Logs** tab in the browser to see the recorded inputs, messages and responses of your chat.

ðŸŽ‰ Now that you have chat messages flowing through your project you can start to log your end user feedback to evaluate and improve your models.
