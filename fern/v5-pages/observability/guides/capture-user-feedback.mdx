---
subtitle: In this guide, we show how to record feedback on generations from your users using the Humanloop Python SDK. This allows you to monitor how your generations perform with your users.
description: Learn how to record user feedback on your generated Prompt Logs using the Humanloop SDK.
image: https://humanloop.com/assets/docs/social-image.png
---

This guide shows how to use the Humanloop SDK to record end-user feedback on Logs.

Different use-cases and user interfaces may require different kinds of feedback that need to be mapped to the appropriate end user interaction.
There are broadly 3 important kinds of feedback:

1. **Explicit feedback**: these are purposeful actions to review the generations. For example, â€˜thumbs up/downâ€™ button presses.
2. **Implicit feedback**: indirect actions taken by your users may signal whether the generation was good or bad, for example, whether the user â€˜copiedâ€™ the generation, â€˜saved itâ€™ or â€˜dismissed itâ€™ (which is negative feedback).
3. **Free-form feedback**: Corrections and explanations provided by the end-user on the generation.

You should create Human Evaluators structured to capture the feedback you need.
For example, a Human Evaluator with return type "text" can be used to capture free-form feedback, while a Human Evaluator with return type "multi_select" can be used to capture user actions
that provide implicit feedback.

If you have not done so, you can follow our guide to [create a Human Evaluator](/docs/v5/evaluation/guides/human-evaluator) to set up the appropriate feedback schema.

## Prerequisites

- You already have a Prompt â€” if not, please follow our [Prompt creation](/docs/guides/create-prompt) guide first.
- You have created a Human Evaluator. This can be done by following the steps in our guide to [Human Evaluator creation](/docs/v5/evaluation/guides/human-evaluation#creating-a-human-evaluator).

<Markdown src="../../../snippets/setup-sdk.mdx" />


## Attach Human Evaluator to enable feedback

First, attach the relevant Human Evaluators to the Prompt to collect feedback on the its Logs.

<Steps>

### Go to the Prompt's dashboard

### Open the Monitoring Dialog

### Add the Human Evaluator

</Steps>


## Record feedback against a Log by its ID

1. Extract the Log ID from the `humanloop.prompts.call()` response.

```python
log = humanloop.prompts.call(
    version_id="prv_qNeXZp9P6T7kdnMIBHIOV",
    path="persona",
    messages=[{"role": "user", "content": "What really happened at Roswell?"}],
    inputs={"person": "Trump"},
)
log_id = log.id
```

2. Call `humanloop.evaluators.log(...)` referencing the above Log ID as `parent_id` to record user feedback.  

```python
feedback = client.evaluators.log(
    # Pass the `log_id` from the previous step to indicate the Log to record feedback against
    parent_id=log_id,
    # We're recording feedback using the "rating" Human Evaluator,
    # which has 2 options: "good" and "bad".
    path="rating",
    judgment="good",
)

# You can also include the source of the feedback when recording it with the `user` parameter.
# This example also demonstrate how to record feedback using a multi-select Human Evaluator.
feedback_2 = client.evaluators.log(
    parent_id=log_id,
    # Pass `user` to record the who provided the feedback
    user="user_123",
    # Here, we're recording feedback against an "issues" Human Evaluator,
    # which is of type `multi_select` and has multiple options to choose from.
    path="issues",
    judgment=["grammar", "relevance"],
)

# You can also record corrections as free-form text feedback
feedback_3 = client.evaluators.log(
    parent_id=log_id,
    path="correction",
    judgment="NOTHING happened at Roswell, folks! Fake News media pushing ALIEN conspiracy theories. SAD! "
    + "I know Area 51, have the best aliens. Roswell? Total hoax! Believe me. ðŸ‘½ðŸš« #Roswell #FakeNews",
)
```

If the user removes their feedback, you can record this by passing `judgment=None` to the `client.evaluators.log()` method.


## Viewing feedback

### Viewing Feedback applied to Logs

The feedback recorded for each Log can be viewed in the **Logs** tab of your Prompt.

<img src="../../../assets/images/b89e24b-image.png" />

Your internal users can also apply feedback to the Logs directly from the **Logs** table.


### Viewing Feedback for an Evaluator

Alternatively, you can view all feedback recorded for a specific Evaluator in the **Logs** tab of the Evaluator.
This will display all feedback recorded for the Evaluator across all other Files.

```

<img src="../../../assets/images/0cd4cd2-image.png" />

