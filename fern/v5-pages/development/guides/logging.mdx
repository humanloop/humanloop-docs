---
subtitle: How to log generations from any large language model (LLM) to Humanloop
description: Learn how to create a Prompt in Humanloop using the UI or SDK, version it, and use it to generate responses from your AI models. Prompt management is a key part of the Humanloop platform.
image: https://app.buildwithfern.com/_next/image?url=https%3A%2F%2Ffdr-prod-docs-files-public.s3.amazonaws.com%2Fhttps%3A%2F%2Fhumanloop.docs.buildwithfern.com%2F2024-05-31T20%3A04%3A53.218Z%2Fassets%2Fimages%2Faae5149-Screenshot_2024-04-04_at_22.36.38.png&w=3840&q=75
---

This guide will show you how to capture the [Logs](/docs/concepts/logs) of your LLM calls into Humanloop.

The easiest way to log LLM generations to Humanloop is to use the `Prompt.call()` method (see the guide on [Calling a Prompt](/docs/development/guides/call-prompt)). You will only need to supply prompt ID and the inputs needed by the prompt template, and the endpoint, will handle fetching the latest template, making the LLM call and logging the result.

However, there may be scenarios that you wish to manage the LLM provider calls directly in your own code instead of relying on Humanloop. For example, you may be using an LLM provider that is not directly supported by Humanloop such as a custom self-hosted model, or you may want to avoid adding Humanloop to the critical path of the LLM API calls.

## Prerequisites

- You already have a Prompt — if not, please follow our [Prompt creation](/docs/guides/create-prompt) guide first.

<Markdown src="../../../snippets/setup-sdk.mdx" />

## Log data to your project

To log LLM generations to Humanloop, you will need to make a call to the `/prompts/log` endpoint.

Note that you can either specify a version of the Prompt you are logging against - in which case you will need to take care that you are supplying the correct version ID and inputs. Or you can supply the full prompt and a new version will be created if it has not been seen before.

<Steps>

### Get your Prompt

Fetch a Prompt from Humanloop by specifying the ID. You can ignore this step if your prompts are created dynamically in code.

<EndpointRequestSnippet endpoint="GET /prompts/:id" />

<EndpointResponseSnippet endpoint="GET /prompts/:id" />

### Call your Prompt

This can be your own model, or any other LLM provider. Here is an example of calling OpenAI:

```python
import openai

client = openai.OpenAI(api_key="<YOUR OPENAI API KEY>")

messages = [{ "role": "user", "content": "Say this is a test" }]

chat_completion = client.chat.completions.create(
    messages=messages,
    model=config.model,
  	temperature=config.temperature
)

# Parse the output from the OpenAI response.
output = chat_completion.choices[0].message.content
```

### Log the result

<EndpointRequestSnippet endpoint="POST /prompts/log" />

```python
from humanloop import Humanloop
import openai

# Initialize Humanloop with your API key
humanloop = Humanloop(api_key="<YOUR Humanloop API KEY>")

project_id = "<YOUR PROJECT ID>"

config = humanloop.projects.get_active_config(id=project_id).config

client = openai.OpenAI(
    # defaults to os.environ.get("OPENAI_API_KEY")
    api_key="<YOUR OPENAI API KEY>",
)

messages = [
    {
        "role": "user",
        "content": "Say this is a test",
    }
]

chat_completion = client.chat.completions.create(
    messages=messages,
    model=config.model,
  	temperature=config.temperature
)

# Parse the output from the OpenAI response.
output = chat_completion.choices[0].message.content

# Log the inputs, outputs and config to your project.
log_response = humanloop.log(
    project_id=project_id,
    messages=messages
    output=output,
    config_id=config.id
)

# Use this ID to associate feedback received later to this datapoint.
data_id = log_response.id
```

#### The process of capturing feedback then uses the returned `data_id` as before.

See our [guide on capturing user feedback](./capture-user-feedback).

#### You can also log immediate feedback alongside the input and outputs:

```
# Log the inputs, outputs and model config to your project.
log_response = humanloop.log(
    project_id=project_id,
    messages=messages
    output=output,
    config_id=config.id,
    feedback={"type": "rating", "value": "good"}
)
```

</Steps>

<Tip title="Hugging Face Example">
Note that you can also use a similar pattern for non-OpenAI LLM providers. For example, logging results from Hugging Face’s Inference API:
 
```python
import requests
from humanloop import Humanloop
 
# Initialize the SDK with your Humanloop API key
humanloop = Humanloop(api_key="<YOUR Humanloop API KEY>")
 
# Make a generation using the Hugging Face Inference API.
response = requests.post(
    "https://api-inference.huggingface.co/models/gpt2",
    headers={"Authorization": f"Bearer {<YOUR HUGGING FACE API TOKEN>}"},
    json={
        "inputs": "Answer the following question like Paul Graham from YCombinator:\n"
        "How should I think about competition for my startup?",
        "parameters": {
            "temperature": 0.2,
            "return_full_text": False,  # Otherwise, Hugging Face will return the prompt as part of the generation.
        },
    },
).json()

# Parse the output from the Hugging Face response.

output = response[0]["generated_text"]

# Log the inputs, outputs and model config to your project.

log_response = humanloop.log(
project=project_id,
inputs={"question": "How should I think about competition for my startup?"},
output=output,
model_config={
"model": "gpt2",
"prompt_template": "Answer the following question like Paul Graham from YCombinator:\n{{question}}",
"temperature": 0.2,
},
)

```
</Tip>
```
