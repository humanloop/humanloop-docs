---
subtitle: A walkthrough for setting up subject-matter experts to evaluate your LLM outputs.
description: Learn how to use Humanloop to set up and manage human evaluations for your large language model use-cases.
image: https://humanloop.com/assets/docs/social-image.png
---


By attaching Human Evaluators to your Evaluations, you can collect annotations from your subject-matter experts
to evaluate the quality of your Prompts' outputs.

## Prerequisites

- You are familiar with setting up Evaluations in Humanloop. See our guide to creating [Evaluations](/docs/v5/evaluation/guides/run-evaluation).
- You have set up a Human Evaluator appropriate for your use-case. See our guide to [creating a Human Evaluator](/docs/v5/evaluation/guides/human-evaluator).


## Using a Human Evaluator in an Evaluation


<Steps>

### Create a new Evaluation

- Go to the **Evaluations** tab of a Prompt.
- Click **Evaluate** in the top-right corner.
- Set up your Evaluation by selecting a Dataset and some Prompt versions to evaluate. See TODO: Guide to Evaluations for more details
- Click the **+ Evaluator** button to add a Human Evaluator to the Evaluation. This will bring up a dialog where you can select the 
Human Evaluator you created earlier. Within this dialog, select the "Tone" Evaluator, and then select its latest version which should be at the top.
- Click **+ Choose** to add the Evaluator to the Evaluation.
- Click **Save** to create the Evaluation.


### Apply feedback to generated Logs

### Review feedback stats

</Steps>










Move below section to capture-user-feedback

## Setting up a Human Evaluator for feedback

You can attach Human Evaluators to Prompts to collect feedback on the Prompt's Logs.

<Steps>

### Go to the Prompt's dashboard


### Open the Monitoring Dialog

### Add the Human Evaluator


</Steps>
