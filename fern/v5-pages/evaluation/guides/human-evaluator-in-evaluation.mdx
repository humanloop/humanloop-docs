---
subtitle: A walkthrough for setting up Human Evaluators in Evaluations to allow subject-matter experts to evaluate your LLM outputs.
description: Learn how to 
image: https://humanloop.com/assets/docs/social-image.png
---


By attaching Human Evaluators to your Evaluations, you can collect annotations from your subject-matter experts
to evaluate the quality of your Prompts' outputs.

## Prerequisites

- You have set up a Human Evaluator appropriate for your use-case. If not, follow our guide to [create a Human Evaluator](./human-evaluator).
- You are familiar with setting up Evaluations in Humanloop. See our guide to creating [Evaluations](./run-evaluation).


## Using a Human Evaluator in an Evaluation

<Steps>

### Create a new Evaluation

The steps in this section are similar to the steps in our guide to [Running an Evaluation in the UI](/docs/v5/evaluation/guides/run-evaluation#run-an-evaluation-via-ui)

- Go to the **Evaluations** tab of a Prompt.
- Click **Evaluate** in the top-right corner.
- Set up your Evaluation by selecting a Dataset and some Prompt versions to evaluate. See TODO: Guide to Evaluations for more details
- Click the **+ Evaluator** button to add a Human Evaluator to the Evaluation. This will bring up a dialog where you can select the 
Human Evaluator you created earlier. Within this dialog, select the "Tone" Evaluator, and then select its latest version which should be at the top.
- Click **+ Choose** to add the Evaluator to the Evaluation.
- Click **Save** to create the Evaluation.


### Apply feedback to generated Logs

### Review feedback stats

</Steps>
